import "./std/io" as io
import "./std/sys" as sys
import "./std/testing" as testing

import "./lexer" as lexer
import "./peeker" as peeker


int main() {
    t := testing.init("lexer.test.bn")

    testing.setup(t, "parses EOF token")
    testing.run(t, should_parse_empty_file(t))

    testing.setup(t, "parses single number")
    testing.run(t, should_parse_int_number(t))

    testing.setup(t, "parses consecutive numbers")
    testing.run(t, should_parse_consecutive_numbers(t))

    testing.setup(t, "ignores whitespace tokens")
    testing.run(t, should_ignore_whitespaces(t))

    testing.setup(t, "parses words token")
    testing.run(t, should_parse_words_token(t))

    testing.setup(t, "parses parens operators")
    testing.run(t, should_parse_parens_operators(t))

    testing.setup(t, "parses curly brackets")
    testing.run(t, should_parse_curly_brackets(t))

    testing.setup(t, "parses list of tokens")
    testing.run(t, should_parse_list_of_tokens(t))

    testing.setup(t, "parses STRING token")
    testing.run(t, should_parse_string_token(t))

    testing.setup(t, "parses CHR token")
    testing.run(t, should_parse_chr_token(t))

    testing.setup(t, "parses NL token")
    testing.run(t, should_parse_nl_token(t))

    testing.setup(t, "parses COMMENT tokens")
    testing.run(t, should_parse_comment_tokens(t))

    testing.setup(t, "parses ... as vararg")
    testing.run(t, parses_dots_as_vararg(t))

    testing.setup(t, "test_parse_not_equal_as_single_token")
    testing.run(t, test_parse_not_equal_as_single_token(t))

    testing.cleanup(t)
    return 0
}

peeker.PeekerInfo* new_peeker(chr* file_content) {
    f := testing.tempfile(file_content)
    f_no := io.fileno(f)
    p := peeker.new(f_no)

    return p
}


int should_parse_empty_file(testing.T* t) {
    p := new_peeker("")
    token_list := lexer.tokenize(p, true)

    testing.assert_equal_str(t, "EOF", token_list.type)
    testing.assert_equal_str(t, "", token_list.value)
    testing.assert_null(t, (chr*) token_list.next)
    testing.assert_null(t, (chr*) token_list.prev)

    sys.free((chr*) p)
    return 0
}

int should_parse_int_number(testing.T* t) {
    p := new_peeker("1")
    token_list := lexer.tokenize(p, true)

    testing.assert_equal_str(t, "NUMBER", token_list.type)
    testing.assert_equal_str(t, "1", token_list.value)
    testing.assert_null(t, (chr*) token_list.prev)
    testing.assert_not_null(t, (chr*) token_list.next)

    -- NOTE: could crash the test
    testing.assert_null(t, (chr*) token_list.next.next)
    testing.assert_equal_str(t, "NUMBER", token_list.next.prev.type)

    sys.free((chr*) p)
    return 0
}

int should_parse_consecutive_numbers(testing.T* t) {
    p := new_peeker("123")
    token_list := lexer.tokenize(p, true)

    testing.assert_equal_str(t, "NUMBER", token_list.type)
    testing.assert_equal_str(t, "123", token_list.value)

    sys.free((chr*) p)
    return 0
}

int should_ignore_whitespaces(testing.T* t) {
    p := new_peeker(" \09")
    token_list := lexer.tokenize(p, true)

    testing.assert_equal_str(t, "EOF", token_list.type)
    testing.assert_equal_str(t, "", token_list.value)
    testing.assert_null(t, (chr*) token_list.next)
    testing.assert_null(t, (chr*) token_list.prev)

    sys.free((chr*) p)
    return 0
}

int should_parse_words_token(testing.T* t) {
    p := new_peeker("sAmp_le")
    token_list := lexer.tokenize(p, true)

    testing.assert_equal_str(t, "WORD", token_list.type)
    testing.assert_equal_str(t, "sAmp_le", token_list.value)

    sys.free((chr*) p)
    return 0
}

int should_parse_parens_operators(testing.T* t) {
    p := new_peeker("()")
    token_list := lexer.tokenize(p, true)

    testing.assert_equal_str(t, "OPERATOR", token_list.type)
    testing.assert_equal_str(t, "(", token_list.value)

    testing.assert_equal_str(t, "OPERATOR", token_list.next.type)
    testing.assert_equal_str(t, ")", token_list.next.value)
    sys.free((chr*) p)
    return 0
}

int should_parse_curly_brackets(testing.T* t) {
    p := new_peeker("{}")
    token_list := lexer.tokenize(p, true)

    testing.assert_equal_str(t, "OPERATOR", token_list.type)
    testing.assert_equal_str(t, "{", token_list.value)

    testing.assert_equal_str(t, "OPERATOR", token_list.next.type)
    testing.assert_equal_str(t, "}", token_list.next.value)
    sys.free((chr*) p)
    return 0
}

int should_parse_list_of_tokens(testing.T* t) {
    p := new_peeker("a: b")
    token_list := lexer.tokenize(p, true)

    testing.assert_equal_str(t, "a", token_list.value)
    testing.assert_equal_str(t, ":", token_list.next.value)
    testing.assert_equal_str(t, "b", token_list.next.next.value)
    return 0
}

int should_parse_string_token(testing.T* t) {
    p := new_peeker("\22b\22")
    token_list := lexer.tokenize(p, true)

    testing.assert_equal_str(t, "STRING", token_list.type)
    testing.assert_equal_str(t, "\22b\22", token_list.value)

    sys.free((chr*) p)
    sys.free((chr*) token_list)
    return 0
}

int should_parse_nl_token(testing.T* t) {
    p := new_peeker("\0A")
    token_list := lexer.tokenize(p, true)

    testing.assert_equal_str(t, "NL", token_list.type)
    testing.assert_equal_str(t, "\0A", token_list.value)
    return 0
}


int should_parse_comment_tokens(testing.T* t) {
    p := new_peeker("-- sample")
    token_list := lexer.tokenize(p, true)

    testing.assert_equal_str(t, "COMMENT", token_list.type)
    testing.assert_equal_str(t, "-- sample", token_list.value)

    sys.free((chr*) p)
    sys.free((chr*) token_list)
    return 0
}

int should_parse_chr_token(testing.T* t) {
    p := new_peeker("'a'")
    token_list := lexer.tokenize(p, true)

    testing.assert_equal_str(t, "CHR", token_list.type)
    testing.assert_equal_str(t, "'a'", token_list.value)

    sys.free((chr*) p)
    sys.free((chr*) token_list)
    return 0
}

int parses_dots_as_vararg(testing.T* t) {
    p := new_peeker("... a")
    token_list := lexer.tokenize(p, true)

    testing.assert_equal_str(t, "KEYWORD", token_list.type)
    testing.assert_equal_str(t, "...", token_list.value)

    testing.assert_equal_str(t, "WORD", token_list.next.type)
    testing.assert_equal_str(t, "a", token_list.next.value)

    sys.free((chr*) p)
    sys.free((chr*) token_list)
    return 0
}

int test_parse_not_equal_as_single_token(testing.T* t) {
    p := new_peeker("!=")
    token_list := lexer.tokenize(p, true)

    testing.assert_equal_str(t, "OPERATOR", token_list.type)
    testing.assert_equal_str(t, "!=", token_list.value)

    return 0
}
