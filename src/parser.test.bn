import "./std/io" as io
import "./std/sys" as sys
import "./std/testing" as testing
import "./parser" as parser
import "./lexer" as lexer
import "./peeker" as peeker
import "./errors" as errors


int main() {
    t := testing.init("parser.test.bn")

    testing.setup(t, "parses one word matcher")
    testing.run(t, should_parse_one_word_matcher(t))

    testing.setup(t, "should_parse_string_as_value_matcher")
    testing.run(t, should_parse_string_as_value_matcher(t))

    testing.setup(t, "should_parse_lowercase_words_as_rule_alias")
    testing.run(t, should_parse_string_as_value_matcher(t))

    testing.setup(t, "parses multiple words matcher")
    testing.run(t, should_parse_two_consecutive_tokens(t))

    testing.setup(t, "should parse catenated matchers")
    testing.run(t, should_parse_catenated_matchers(t))

    testing.setup(t, "parses repeating condition +")
    testing.run(t, should_parse_repeating_condition_plus(t))

    testing.setup(t, "parses repeating condition *")
    testing.run(t, should_parse_repeating_condition_times(t))

    testing.setup(t, "should parse or condition")
    testing.run(t, should_parse_or_condition(t))

    testing.setup(t, "parses repeating condition ?")
    testing.run(t, should_parse_repeating_condition_question(t))

    testing.setup(t, "parses rule")
    testing.run(t, should_parse_matching_rule(t))

    testing.setup(t, "parses multiple rules")
    testing.run(t, should_parse_multiple_rules(t))

    testing.setup(t, "parses node using rule")
    testing.run(t, should_parse_node_using_rule(t))

    testing.setup(t, "should_parse_node_using_rules_with_words")
    testing.run(t, should_parse_node_using_rules_with_words(t))

    testing.setup(t, "should_parse_node_using_alias_rules")
    testing.run(t, should_parse_node_using_alias_rules(t))

    testing.setup(t, "should_parse_alternative_rules")
    testing.run(t, should_parse_alternative_rules(t))

    testing.setup(t, "should_rollback_if_rule_does_not_match")
    testing.run(t, should_rollback_if_rule_does_not_match(t))

    testing.setup(t, "should_match_with_repeating_conditions")
    testing.run(t, should_match_with_repeating_conditions(t))

    testing.setup(t, "should_parse_with_optional_parser")
    testing.run(t, should_parse_with_optional_parser(t))

    testing.setup(t, "test_optional_operator_matches_records")
    testing.run(t, test_optional_operator_matches_records(t))

    testing.setup(t, "test_optionals_dont_interrupt_matching_chain")
    testing.run(t, test_optionals_dont_interrupt_matching_chain(t))

    testing.setup(t, "should_report_error_if_element_is_not_matched")
    testing.run(t, should_report_error_if_element_is_not_matched(t))

    testing.setup(t, "should_fail_if_source_is_not_fully_parsed")
    testing.run(t, should_fail_if_source_is_not_fully_parsed(t))

    testing.setup(t, "test_fully_compares_words_by_value")
    testing.run(t, test_fully_compares_words_by_value(t))

    testing.cleanup(t)
    return 0
}

parser.Matcher* build_matcher(chr* pattern) {
    f := testing.tempfile(pattern)
    ctx := parser.new_context(f)
    matcher := parser.parse_matcher(ctx)

    sys.free((chr*) ctx)
    return matcher
}

int should_parse_one_word_matcher(testing.T* t) {
    matcher := build_matcher("NUMBER")

    -- testing.assert_not_null(t, (chr*) matcher)
    testing.assert_null(t, (chr*) matcher.alternative)
    testing.assert_equal_chr(t, '1', matcher.times)
    testing.assert_not_null(t, (chr*) matcher.querys)

    qs := matcher.querys
    testing.assert_equal_chr(t, 't', qs.type)
    testing.assert_null(t, (chr*) qs.next)
    testing.assert_equal_str(t, "NUMBER", qs.value)
    return 0
}

int should_parse_lowercase_words_as_rule_alias(testing.T* t) {
    matcher := build_matcher("number")
    testing.assert_equal_chr(t, 'a', matcher.querys.type)
    return 0
}

int should_parse_string_as_value_matcher(testing.T* t) {
    matcher := build_matcher("\22NUMBER\22")

    testing.assert_equal_chr(t, 'v', matcher.querys.type)
    return 0
}

int should_parse_two_consecutive_tokens(testing.T* t) {
    matcher := build_matcher("a b")

    testing.assert_null(t, (chr*) matcher.alternative)

    qs := matcher.querys
    testing.assert_equal_str(t, "a", qs.value)
    testing.assert_not_null(t, (chr*) qs.next)
    if qs.next == null {
        return 0
    }
    testing.assert_equal_str(t, "b", qs.next.value)

    return 0
}

int should_parse_catenated_matchers(testing.T* t) {
    matcher := build_matcher("(a) (b)")
    testing.assert_not_null(t, (chr*) matcher.next)
    return 0
}

int should_parse_repeating_condition_plus(testing.T* t) {
    matcher := build_matcher("(a) + ")
    testing.assert_equal_chr(t, '+', matcher.times)
    return 0
}

int should_parse_repeating_condition_times(testing.T* t) {
    matcher := build_matcher("(a) * ")
    testing.assert_equal_chr(t, '*', matcher.times)
    return 0
}

int should_parse_or_condition(testing.T* t) {
    matcher := build_matcher("(a | b)")

    testing.assert_not_null(t, (chr*) matcher.querys)
    testing.assert_not_null(t, (chr*) matcher.alternative)
    testing.assert_null(t, (chr*) matcher.alternative.alternative)
    testing.assert_equal_chr(t, '1', matcher.times)

    return 0
}

int should_parse_repeating_condition_question(testing.T* t) {
    matcher := build_matcher("(a b)?")
    testing.assert_not_null(t, (chr*) matcher.querys)
    testing.assert_null(t, (chr*) matcher.alternative)
    testing.assert_equal_chr(t, '?', matcher.times)
    return 0
}

int should_parse_matching_rule(testing.T* t) {
    f := testing.tempfile("rule_name: example pattern")
    ctx := parser.new_context(f)
    rule := parser.parse_rule(ctx)
    sys.free((chr*) ctx)

    testing.assert_equal_str(t, "rule_name", rule.name)
    testing.assert_equal_str(t, "example", rule.matcher.querys.value)
    testing.assert_equal_str(t, "pattern", rule.matcher.querys.next.value)
    return 0
}

int should_parse_multiple_rules(testing.T* t) {
    f := testing.tempfile("rule_name: first a\0Asecond: b")
    ctx := parser.new_context(f)
    grammar := parser.parse_grammar(ctx)

    testing.assert_equal_str(t, ctx.current_token.type, "EOF")
    sys.free((chr*) ctx)

    rule0 := grammar[0]
    testing.assert_equal_str(t, "rule_name", rule0.name)

    rule1 := grammar[1]
    testing.assert_equal_str(t, "second", rule1.name)
    return 0
}


Rule** gen_grammar(chr* grammar_content) {
    f := testing.tempfile(grammar_content)
    ctx := parser.new_context(f)
    grammar := parser.parse_grammar(ctx)
    sys.free((chr*) ctx)
    return grammar
}

Node* parse_file(Rule** grammar, chr* file_content) {
    source := testing.tempfile(file_content)
    fd := io.fileno(source)
    p := peeker.new(fd)
    source_tokens := lexer.tokenize(p, false)
    sys.free((chr*) p)

    result := parser.ast(grammar, "start", source_tokens)
    rv := result.node
    sys.free((chr*) result)
    return rv
}

int should_parse_node_using_rule(testing.T* t) {
    grammar := gen_grammar("start: \22a\22 \22b\22")
    node := parse_file(grammar, "a b")

    -- asserts the following
    -- {
    --   type: "start", value: "", line: 1, char_of_line: 1,
    --   children: [
    --     { type: "leaf", value: "a", line: 1, char_of_line: 1, children: null}
    --     { type: "leaf", value: "b", line: 1, char_of_line: 3, children: null}
    --   ]
    -- }

    testing.assert_not_null(t, (chr*) node)
    testing.assert_equal_str(t, "start", node.type)
    testing.assert_not_null(t, (chr*) node.children)
    testing.assert_equal_str(t, "a", node.children.value)
    testing.assert_equal_str(t, "WORD", node.children.type)

    testing.assert_not_null(t, (chr*) node.children.next)
    testing.assert_equal_str(t, "b", node.children.next.value)
    testing.assert_equal_str(t, "WORD", node.children.next.type)
    testing.assert_equal_int(t, 3, node.children.next.char_of_line)
    return 0
}

int should_parse_node_using_rules_with_words(testing.T* t) {
    grammar := gen_grammar("start: STRING WORD")
    node := parse_file(grammar, "\22a\22 b")

    testing.assert_not_null(t, (chr*) node)

    testing.assert_equal_str(t, "STRING", node.children.type)
    testing.assert_equal_str(t, "\22a\22", node.children.value)

    testing.assert_equal_str(t, "WORD", node.children.next.type)
    testing.assert_equal_str(t, "b", node.children.next.value)
    return 0
}

int should_parse_node_using_alias_rules(testing.T* t) {
    grammar := gen_grammar("start: a a\0Aa: WORD")
    node := parse_file(grammar, "a b")

    testing.assert_not_null(t, (chr*) node.children)
    if node.children == null {
        return 0
    }
    testing.assert_equal_str(t, "a", node.children.type)
    -- testing.assert_equal_str(t, "WORD", node.children.type)
    testing.assert_not_null(t, (chr*) node.children.children)
    return 0
}

int should_parse_alternative_rules(testing.T* t) {
    grammar := gen_grammar("start: ( WORD | STRING )")
    node := parse_file(grammar, "\22sample\22")

    testing.assert_not_null(t, (chr*) node)
    testing.assert_not_null(t, (chr*) node.children)
    if node.children == null {
        return 0
    }
    testing.assert_equal_str(t, "STRING", node.children.type)

    return 0
}

int should_rollback_if_rule_does_not_match(testing.T* t) {
    grammar := gen_grammar("start: ( conda | condb )\0Aconda: WORD STRING\0Acondb: WORD WORD")
    node := parse_file(grammar, "a b")

    testing.assert_not_null(t, (chr*) node)
    testing.assert_not_null(t, (chr*) node.children)
    if node.children == null {
        return 0
    }
    testing.assert_equal_str(t, "condb", node.children.type)
    testing.assert_not_null(t, (chr*) node.children.children)
    if node.children.children == null {
        return 0
    }
    testing.assert_equal_str(t, "a", node.children.children.value)
    testing.assert_not_null(t, (chr*) node.children.children.next)
    testing.assert_equal_str(t, "b", node.children.children.next.value)

    return 0
}

int should_match_with_repeating_conditions(testing.T* t) {
    grammar := gen_grammar("start: (WORD)+")
    node := parse_file(grammar, "a b c")

    testing.assert_not_null(t, (chr*) node.children)
    if node.children == null {
        return 0
    }
    testing.assert_not_null(t, (chr*) node.children.next)
    if node.children.next == null {
        return 0
    }
    testing.assert_not_null(t, (chr*) node.children.next.next)
    return 0
}

int should_parse_with_optional_parser(testing.T* t) {
    grammar := gen_grammar("start: WORD ( STRING )?")
    node := parse_file(grammar, "a")

    testing.assert_not_null(t, (chr*) node)
    if node == null {
        return 0
    }
    return 0
}

int test_optional_operator_matches_records(testing.T* t) {
    grammar := gen_grammar("start: WORD (WORD)?")
    node := parse_file(grammar, "a b")

    testing.assert_not_null(t, (chr*) node.children)
    if node.children == null {
        return 0
    }

    testing.assert_not_null(t, (chr*) node.children.next)
    return 0
}

int test_optionals_dont_interrupt_matching_chain(testing.T* t) {
    grammar := gen_grammar("start: WORD (\22.\22 start)?")
    source := testing.tempfile("a.b")
    fd := io.fileno(source)
    p := peeker.new(fd)
    source_tokens := lexer.tokenize(p, false)
    sys.free((chr*) p)

    result := parser.ast(grammar, "start", source_tokens)
    testing.assert_null(t, (chr*) result.err)
    if result.err != null {
        errors.report(result.err)
        sys.free((chr*) result.err)
    }
    return 0
}

int should_report_error_if_element_is_not_matched(testing.T* t) {
    grammar := gen_grammar("start: ( WORD WORD STRING | WORD STRING )")
    source := testing.tempfile("a a a")
    fd := io.fileno(source)
    p := peeker.new(fd)
    source_tokens := lexer.tokenize(p, false)
    sys.free((chr*) p)

    result := parser.ast(grammar, "start", source_tokens)
    testing.assert_not_null(t, (chr*) result.err)
    testing.assert_equal_int(t, 5, result.err.char_of_line)
    return 0
}


int should_fail_if_source_is_not_fully_parsed(testing.T* t) {
    grammar := gen_grammar("start: \22a\22")
    source := testing.tempfile("a b")
    fd := io.fileno(source)
    p := peeker.new(fd)
    source_tokens := lexer.tokenize(p, false)
    sys.free((chr*) p)

    result := parser.ast(grammar, "start", source_tokens)
    testing.assert_not_null(t, (chr*) result.err)
    if result.err == null {
        return 0
    }
    testing.assert_equal_int(t, 3, result.err.char_of_line)
    return 0
}

int test_fully_compares_words_by_value(testing.T* t) {
    grammar := gen_grammar("start: \22a\22")
    source := testing.tempfile("alt")
    fd := io.fileno(source)
    p := peeker.new(fd)
    source_tokens := lexer.tokenize(p, false)
    sys.free((chr*) p)
    result := parser.ast(grammar, "start", source_tokens)

    testing.assert_not_null(t, (chr*) result.err)
    return 0
}
