-- breaks up file in one or multiple tokens
import "./std/sys" as sys
import "./std/string" as string
import "./std/io" as io

import "./peeker" as peeker
#import "./errors" as errors


alias Token = struct {
    chr* type,
    chr* value,

    int line,
    int char_of_line,
    chr* filename,

    Token* next,
    Token* prev,
}
-- int Token_size = 192-- 4 * ptr_size + 64
int Token_size = 48


alias ParseCtx = struct {
    chr* filename,
    int line,
    int char_of_line,

    Token* root,
    Token* curr_token,
}
int ParseCtx_size = 160


Token* push_token(ParseCtx* c, str type, value) {
    Token* root = (Token*) sys.malloc(Token_size)
    root.type = type
    root.value = value
    root.filename = c.filename
    root.line = c.line
    root.char_of_line = c.char_of_line
    root.prev = c.curr_token
    root.next = null

    if c.curr_token != null {
        c.curr_token.next = root
    }
    c.curr_token = root

    if c.root == null {
        c.root = root
    }

    return root
}


-- takes a peeker in input, returns the sequence of tokens
-- depending on the first character, start parsing a
-- specific token.
-- Here are the rules:
--
--    [0-9] -> INT_NUMBER
--    [a-zA-Z_] -> WORD
--    ["] -> STRING
--    ['] -> CHR
--    [+-/*=[({])}|] -> OPERATOR or COMMENT
Token* tokenize(peeker.PeekerInfo* p, bool keep_comments) {
    ParseCtx* ctx = (ParseCtx*) sys.malloc(ParseCtx_size)
    ctx.filename = p.filename
    ctx.root = null
    ctx.curr_token = null

    int max_token_size = 128
    chr* buf = (chr*) null

    int idx = 0


    c := peeker.read(p)
    for ; p.eof == false ; {
        ctx.line = p.line
        ctx.char_of_line = p.char_of_line

        if c == '\00' {
            -- eof
            c = peeker.read(p)

        } elif string.is_digit(c) {
            buf = sys.malloc(max_token_size)

            for idx = 0 ; string.is_digit(c); c = peeker.read(p) {
                buf[idx] = c
                idx = idx + 1
                sys.assert(idx < max_token_size, "digit too large")
            }
            buf[idx] = '\00'

            push_token(ctx, "NUMBER", buf)

        } elif string.is_letter(c) | c == '_' {
            buf = sys.malloc(max_token_size)

            for idx = 0 ; string.is_letter(c) | string.is_digit(c) | c == '_'; c = peeker.read(p) {
                buf[idx] = c
                idx = idx + 1
                sys.assert(idx < max_token_size, "digit too large")
            }
            buf[idx] = '\00'
            push_token(ctx, "WORD", buf)

        } elif string.is_whitespace(c) {
            -- ignore whitespaces and tabs for now
            c = peeker.read(p)

        } elif c == '\22' {
            buf = sys.malloc(max_token_size)
            buf[0] = c

            -- read string content
            c = peeker.read(p)
            for idx = 1; c != '\22'; c = peeker.read(p) {
                sys.assert(c != '\00', "lexer: reached end of file while scanning for 'STRING' token")

                sys.assert(idx < max_token_size - 2, "string too large")
                buf[idx] = c
                idx = idx + 1
            }

            -- read terminator character
            buf[idx] = c
            buf[idx + 1] = '\00'
            c = peeker.read(p)

            push_token(ctx, "STRING", buf)
        } elif c == '\27' {
            buf = sys.malloc(max_token_size)
            buf[0] = c
            c = peeker.read(p)
            for idx = 1; c != '\27'; c = peeker.read(p) {
                sys.assert(idx < max_token_size - 2, "char too large")
                buf[idx] = c
                idx = idx + 1
            }
            buf[idx] = c
            buf[idx + 1] = '\00'
            c = peeker.read(p)

            push_token(ctx, "CHR", buf)

        } elif c == '\0A' {
            push_token(ctx, "NL", "\0A")
            c = peeker.read(p)
        } elif c == '-' {
            c = peeker.read(p)
            if c == '-' {
                -- if next token is a '-' start parsing a comment
                -- start parsing a comment block
                buf = sys.malloc(max_token_size)
                buf[0] = '-'
                for idx = 1; c != '\0A' & c != '\00' ; c = peeker.read(p) {
                    buf[idx] = c
                    idx = idx + 1
                }
                buf[idx] = '\00'

                if keep_comments {
                    push_token(ctx, "COMMENT", buf)
                } else {
                    sys.free((chr*) buf)
                }

            } else {
                push_token(ctx, "OPERATOR", "-")
            }
        } else {
            -- check for operator
            if c > '\7E' {
                io.printf("%d:%d error: found non ascii token: '%c' (%d)\0A", ctx.line, ctx.char_of_line, c, c)
                sys.exit(1)
            }

            -- TODO: some operators are composed by two or more characters
            -- ex. != or ==

            prev_c := c

            c = peeker.read(p)

            chr* type = "OPERATOR"
            if prev_c == '=' & c == '=' {
                io.asprintf(&buf, "==")
                c = peeker.read(p)
            } elif prev_c == '!' & c == '=' {
                io.asprintf(&buf, "!=")
                c = peeker.read(p)
            } elif prev_c == '.' & c == '.' {
                -- ensure next token is . otherwise tokenerror raised
                c = peeker.read(p)
                if c != '.' {
                    io.printf("error on ..\0A")
                    -- err := new_error("unrecognized operand ..")
                    sys.exit(1)
                } else {
                    io.asprintf(&buf, "...")
                    type = "KEYWORD"
                }
            } else {
                buf = sys.malloc(2)
                buf[0] = prev_c
                buf[1] = '\00'
            }
            push_token(ctx, type, buf)
        }
    }

    -- set curr_token as eof
    push_token(ctx, "EOF", "")

    Token* root = ctx.root
    sys.free((chr*) ctx)
    return (Token*) root
}

