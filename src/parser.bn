-- :Parser module:
-- builds grammar from syntax file
-- uses grammar to build ast

import "./std/io" as io
import "./std/sys" as sys
import "./std/string" as string
import "./peeker" as peeker
import "./lexer" as lexer
import "./errors" as errors


alias Query = struct {
    -- 'v': .value contains the value of the token
    -- 'a': .value is an alias to a rule
    -- 't': .value contains the type of the token ex: EOF, STRING, CHAR...
    chr type,
    chr* value,

    Query* next,
}
int Query_size = 24 -- 2 * sys.ptr_size + 1


alias Matcher = struct {

    -- list of names to match.
    -- reserved names are for a specific token type, which could be
    -- NUMBER, STRING, WORD, CHAR, EOF
    Query* querys,

    -- how many times this matcher should be applied
    -- [1] just one tim (default)
    -- [+] one or more
    -- [?] one or none
    -- [*] none or more
    chr times,

    -- in case this matcher fails call the following matcher
    Matcher* alternative,
    Matcher* next,
}
int Matcher_size = 32

-- visually represent matcher, used only for debug purposes
void debug_matcher(Matcher* m) {
    -- print querys
    io.printf("{\0A\22querys\22: [")
    for q := m.querys; q != null; q = q.next {
        if q.type == 'v' {
            io.printf("\0A{\22type\22: \22%c\22, \22value\22: %s},", q.type, q.value)
        } else {
            io.printf("\0A{\22type\22: \22%c\22, \22value\22: \22%s\22},", q.type, q.value)
        }
    }
    io.printf("],\0A\22next\22: [\0A")
    if m.next != null {
        debug_matcher(m.next)
    }
    io.printf("],\0A\22alt\22: [\0A")

    if m.alternative != null {
        debug_matcher(m.alternative)
    }
    io.printf("]\0A}\0A")
}

alias Rule = struct {
    chr*      name,
    Matcher*  matcher,
}
int Rule_size = 16 -- matcher size + 1


-- parsing context, it's used to keep track of the current token being
-- parsed. a new parsing context could be obtained starting from an
-- `io.File*` by using the function `new_context`
alias ParsingContext = struct {
    lexer.Token* current_token,
}
int ParsingContext_size = 8


ParsingContext* new_context(io.File* f) {
    ParsingContext* ctx = (ParsingContext*) sys.malloc(ParsingContext_size)

    fd := io.fileno(f)
    p := peeker.new(fd)
    token_list := lexer.tokenize(p, false)
    sys.free((chr*) p)

    ctx.current_token = token_list
    return ctx
}

-- matcher constructor
Matcher* new_matcher() {
    Matcher* m = (Matcher*) sys.malloc(Matcher_size)
    m.times = '1'
    m.alternative = null
    m.next = null
    m.querys = null
    return m
}


-- parse the grammar file, starting from the current "context cursor
-- position"
Matcher* parse_matcher(ParsingContext* ctx) {
    Matcher* m
    if string.strcmp(ctx.current_token.value, "(") == 0 {
        ctx.current_token = ctx.current_token.next
        int max_matchers = 10

        m = parse_matcher(ctx)
        -- ) | EOF

        if string.strcmp(ctx.current_token.value, "|") == 0 {
            -- parse matcher chain
            ctx.current_token = ctx.current_token.next
            last_matcher := m

            for ; string.strcmp(ctx.current_token.value, ")") != 0 & string.strcmp(ctx.current_token.type, "EOF") != 0 ; {
                last_matcher.alternative = parse_matcher(ctx)
                last_matcher = last_matcher.alternative

                if string.strcmp(ctx.current_token.value, "|") != 0 & string.strcmp(ctx.current_token.value, ")") != 0 {
                    io.printf("[%d:%d] malformed grammar, expected | got '%s'\0A", ctx.current_token.line, ctx.current_token.char_of_line, ctx.current_token.value)
                    sys.exit(1)
                }

                if string.strcmp(ctx.current_token.value, "|") == 0 {
                    ctx.current_token = ctx.current_token.next
                }
            }
        }

        if string.strcmp(ctx.current_token.value, ")") != 0 {
            -- panic, wrong grammar
            parser_error(ctx, "open brace not closed")
        }
        ctx.current_token = ctx.current_token.next

        -- parse molteplicity
        if string.strcmp(ctx.current_token.value, "+") == 0 {
            ctx.current_token = ctx.current_token.next
            m.times = '+'
        } elif string.strcmp(ctx.current_token.value, "*") == 0 {
            ctx.current_token = ctx.current_token.next
            m.times = '*'
        } elif string.strcmp(ctx.current_token.value, "?") == 0 {
            ctx.current_token = ctx.current_token.next
            m.times = '?'
        }

    } else {
        m = parse_simple_matcher(ctx)
    }

    -- if a bracket is opened, another matcher should be parsed
    -- this allows for (a b) (c d) or a (b c)? d
    if string.strcmp(ctx.current_token.value, "(") == 0 | string.strcmp(ctx.current_token.type, "WORD") == 0 {
        m.next = parse_matcher(ctx)
    }

    return m
}



Matcher* parse_simple_matcher(ParsingContext* ctx) {
    Matcher* m = new_matcher()

    -- count number of 'words'
    int count
    ptr := ctx.current_token
    for count = 0; string.strcmp(ptr.type, "WORD") == 0 | string.strcmp(ptr.type, "STRING") == 0; {
        count = count + 1
        ptr = ptr.next
    }

    if Query_size > 0 {
        m.querys = (Query*) sys.malloc(Query_size)
    } else {
        m.querys = null
    }

    q := m.querys
    for i := 0; i < count; i = i + 1 {
        if i > 0 {
            q.next = (Query*) sys.malloc(Query_size)
            q = q.next
        }

        -- STRING it's used to match by token value
        -- WORD UPPERCASE is used to match by token type
        -- WORD LOWERCASE is used to alias another rule
        if string.strcmp(ctx.current_token.type, "STRING") == 0 {
            q.type = 'v'
        } elif string.is_lower(ctx.current_token.value) {
            q.type = 'a'
        } elif string.is_upper(ctx.current_token.value) {
            q.type = 't'
        } else {
            io.printf("rule name: '%s'\0A", ctx.current_token.value)
            parser_error(ctx, "Some characters are not allowed in this rule name")
        }
        q.value = ctx.current_token.value
        q.next = null
        ctx.current_token = ctx.current_token.next
    }

    return m
}


Rule* parse_rule(ParsingContext* ctx) {
    rule_name := ctx.current_token.value
    if string.strcmp(ctx.current_token.type, "WORD") != 0 {
        io.printf("Identifier: {type: %s, value: '%s'}\0A", ctx.current_token.type, ctx.current_token.value)
        parser_error(ctx, "rule identifier must be a WORD")
    }
    ctx.current_token = ctx.current_token.next

    if string.strcmp(ctx.current_token.value, ":") != 0 {
        parser_error(ctx, "expected : after rule name")
    }
    ctx.current_token = ctx.current_token.next

    Rule* rule = (Rule*) sys.malloc(Rule_size)
    rule.name = rule_name


    rule.matcher = parse_matcher(ctx)
    return rule
}

void parser_error(ParsingContext* ctx, chr* error) {
    e := errors.from(ctx.current_token, error)
    errors.report(e)
    sys.exit(1)
}

-- a grammar is buildt by one or more rules,
-- the entire grammar describes how the ast is buildt
Rule** parse_grammar(ParsingContext* ctx) {
    int max_rules = 40
    Rule** grammar = (Rule**) sys.malloc(sys.ptr_size * max_rules)

    for i := 0; string.strcmp(ctx.current_token.type, "EOF") != 0; i = i + 1 {
        grammar[i] = parse_rule(ctx)

        -- remove empty rules
        for ; string.strcmp(ctx.current_token.value, "\0A") == 0; {
            ctx.current_token = ctx.current_token.next
        }
    }

    return (Rule**) grammar
}


-- basic node for 'ast' representation
alias Node = struct {
    chr* type,
    chr* value,
    chr* filename,

    int line,
    int char_of_line,

    Node* parent,

    Node* children,
    Node* next,      -- required for a list of child nodes
}
int Node_size = 56

Node* new_node(lexer.Token* t, chr* type, chr* value) {
    node := (Node*) sys.malloc(Node_size)
    node.type = type
    node.value = value
    node.line = t.line
    node.filename = t.filename
    node.char_of_line = t.char_of_line
    node.parent = null
    node.next = null
    node.children = null
    return node
}

-- append child to a node
void child_append(Node* parent, Node* child) {
    if parent.children == null {
        parent.children = child
    } else {
        c := parent.children
        for ; c.next != null; c = c.next {
        }
        c.next = child
    }
    child.next = null
}


-- remove last node's child
void child_pop(Node* parent) {
    if parent.children == null {
        -- no children to pop
        return
    }

    c := parent.children
    if c.next == null {
        -- parent node has only one child
        sys.free((chr*) c)
        parent.children = null
        return
    }

    -- retrive previous to last child
    for ; c.next.next != null; c = c.next {
    }

    sys.free((chr*) c.next)
    c.next = null
}


alias AstContext = struct {
    lexer.Token* current_token,
    Rule** grammar,
}


Rule* _find_rule(Rule** grammar, str rule_name) {
    for i := 0; true ;i = i + 1 {
        rule := grammar[i]
        if string.strcmp(rule.name, rule_name) == 0 {
            return rule
        }
    }
    return (Rule*) null
}


alias ParseResult = struct {
    errors.Error* err,
    Node* node,
}
int ParseResult_size = 16 -- sys.ptr_size * 2


-- mutates base node, assigning to it the parsed results
errors.Error* parse_query(AstContext* c, Matcher* m, Node* base) {
    Node* child
    start_token := c.current_token

    new_children := 0
    errors.Error* err = (errors.Error*) null

    for q := m.querys; q != null & err == null; q = q.next {
        if q.type == 'v' {
            -- match by exact token value
            qv_len := string.strlen(q.value) -2
            tmp_buff := sys.malloc(qv_len + 1)
            io.sscanf(q.value, "\22%[^\22]\22", tmp_buff)

            if string.strncmp(tmp_buff, c.current_token.value, qv_len) == 0 & qv_len == string.strlen(c.current_token.value) {
                -- we have a match on this node
                child = new_node(c.current_token, c.current_token.type, c.current_token.value)
                child_append(base, child)
                new_children = new_children + 1

                c.current_token = c.current_token.next
            } else {
                -- match failure, depending on the parsing mode choose the
                -- appropriate outcome
                token_value := c.current_token.value
                if token_value == null {
                    token_value = "(null)"
                }

                err_fmt := "%s: %s != \22%s\22"
                err_msg := "unable to match token by value"
                err_len := string.strlen(err_msg) + string.strlen(tmp_buff) + string.strlen(token_value) + string.strlen(err_fmt) - 3 * 2 + 1
                err_buf := sys.malloc(err_len)

                io.snprintf(err_buf, err_len, err_fmt, err_msg, tmp_buff, token_value)
                err = errors.from(c.current_token, err_buf)
            }
            sys.free(tmp_buff)

        } elif q.type == 't' {
            -- match by token type
            if string.strcmp(q.value, c.current_token.type) == 0 {
                -- we have a match
                child = new_node(c.current_token, c.current_token.type, c.current_token.value)
                child_append(base, child)
                new_children = new_children + 1
                c.current_token = c.current_token.next
            } else {
                err_fmt_t := "%s: %s != %s"
                err_msg_t := "unable to match token by type"
                err_len_t := string.strlen(err_fmt_t) + string.strlen(err_msg_t) + string.strlen(q.value) + string.strlen(c.current_token.type)
                err_buf_t := sys.malloc(err_len_t)

                io.snprintf(err_buf_t, err_len_t, err_fmt_t, err_msg_t, q.value, c.current_token.type)
                err = errors.from(c.current_token, err_buf_t)
            }

        } elif q.type == 'a' {
            -- retrieve parse rule
            alias_rule := _find_rule(c.grammar, q.value)
            if alias_rule == null {
                err = errors.from(c.current_token, "unable to retrieve alias rule")
            } else {
                parse_result := parse_to_ast(c, alias_rule)

                if parse_result.err != null {
                    err = parse_result.err
                } else {
                    child_append(base, parse_result.node)
                    new_children = new_children + 1
                }
                sys.free((chr*) parse_result)
            }

        } else {
            sys.assert(false, "parser.bn: query matcher not implemented")
        }
    }

    if err != null {
        -- restore previous token position
        c.current_token = start_token
        for i := 0; i < new_children; i = i + 1 {
            child_pop(base)
        }
    }


    if m.alternative != null & err != null {
        -- ignore errors on or matcher
        new_err := parse_query(c, m.alternative, base)
        if new_err == null {
            err = null
        } else {
            sys.free((chr*) new_err)
        }
    }


    if m.times == '1' {
        -- done
    } elif m.times == '+' & err == null {
        -- it should be ensured that this matcher runs at lest one time.
        -- and it should not run the 'parse_query' with 'm.next'
        -- multiple times, in order to avoid failures.

        -- therefor the execution of the 'next' is delegated to the last one
        -- where the next has a failure
        err = parse_query(c, m, base)
        if err != null {
            sys.free((chr*) err)
            err = null
        } else {
            return (errors.Error*) null
        }

    } elif m.times == '?' & err != null {
        sys.free((chr*) err)
        err = null

    } elif m.times == '*' {
        -- match one or more times
        if err != null {
            sys.free((chr*) err)
            err = null
        } else {
            -- a 'star' matcher ends it's matching when error is not null
            -- when the error is null, the execution is delegated to
            -- at the last match.
            return parse_query(c, m, base)
        }
    }

    if m.next != null & err == null {
        err = parse_query(c, m.next, base)
    }

    return (errors.Error*) err
}


-- start parsing the list of tokens by applying a specific rule
ParseResult* parse_to_ast(AstContext* c, Rule* rule) {
    res := (ParseResult*) sys.malloc(ParseResult_size)
    res.err = null
    res.node = new_node(c.current_token, rule.name, (chr*) null)
    start_match := c.current_token

    -- start matching querys tokens
    res.err = parse_query(c, rule.matcher, res.node)
    return res
}


-- Parse a list of tokens
--
-- :param grammar: list of null terminated grammar rules
-- :param start: starting rule to use for token parsing
-- :param tokens: list of tokens of the language
ParseResult* ast(Rule** grammar, str start, lexer.Token* tokens) {
    -- find starting rule
    Rule* start_matcher = _find_rule(grammar, start)
    sys.assert(start_matcher != null, "Unable to find starting rule in the grammar")

    AstContext c
    c.current_token = tokens
    c.grammar = grammar

    res := parse_to_ast(&c, start_matcher)
    if string.strcmp(c.current_token.type, "EOF") != 0 & res.err == null {
        res.err = errors.from(c.current_token, "syntax parsing ended here")
    }

    return res
}
