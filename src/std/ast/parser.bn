-- :Parser module:
-- builds grammar from syntax file
-- uses grammar to build ast
import "../io" as io
import "../sys" as sys
import "../string" as string
import "./peeker" as peeker
import "./lexer" as lexer
import "./errors" as errors


chr TYPE_VALUE  = 'v'
chr TYPE_ALIAS  = 'a'
chr TYPE_TYPE   = 't'
chr TYPE_IGNORE = '\x00'


alias Matcher = struct {
    -- how many times this matcher should be applied
    -- [1] just one tim (default)
    -- [+] one or more
    -- [?] one or none
    -- [*] none or more
    chr times,
    str tag,

    -- in case this matcher fails call the following matcher
    Matcher* alternative,
    Matcher* next,
    Matcher* submatcher,

    -- names to match.
    -- reserved names are for a specific token type, which could be
    -- NUMBER, STRING, WORD, CHAR, EOF
    -- type of the matcher
    chr type,
    str value,
}

alias Rule = struct {
    str       name,
    str       transformer,
    Matcher*  matcher,
}


-- parsing context, it's used to keep track of the current token being
-- parsed. a new parsing context could be obtained starting from an
-- `io.File*` by using the function `new_context`
alias ParsingContext = struct {
    lexer.Token* current_token,
    errors.Error* err,
}



ParsingContext* new_context(io.File* f) {
    ParsingContext* ctx = (ParsingContext*) sys.malloc(sizeof(ParsingContext))

    fd := io.fileno(f)
    p := peeker.new(fd)
    token_list := lexer.tokenize(p, false)
    sys.free((chr*) p)

    ctx.current_token = token_list
    return ctx
}

-- matcher constructor
Matcher* new_matcher() {
    Matcher* m = (Matcher*) sys.malloc(sizeof(Matcher))
    m.type = TYPE_IGNORE
    m.value = ""
    m.tag = ""
    m.submatcher = null
    m.times = '1'
    m.alternative = null
    m.next = null
    return m
}

str value(lexer.Token* token) {
    if token == null {
        return ""
    }
    return token.value
}

str type(lexer.Token* token) {
    if token == null {
        return ""
    }
    return token.type
}

lexer.Token* next(lexer.Token* token) {
    if token == null {
        return (lexer.Token*) null
    }
    return token.next
}

void append_submatch(Matcher* m, child) {
    if m.submatcher == null {
        m.submatcher = child
        return
    }
    base := m.submatcher
    for ; base.next != null; base = base.next {
    }
    base.next = child
}

-- parse the grammar file, starting from the current "context cursor
-- position"
Matcher* parse_matcher(ParsingContext* ctx) {
    if string.strcmp(ctx.current_token.value, ")") == 0 {
        return (Matcher*) null
    }
    if string.strcmp(ctx.current_token.value, "]") == 0 {
        return (Matcher*) null
    }

    m := new_matcher()

    tmp_submatch := (Matcher*) null
    for ; ctx.current_token != null ; {
        if string.strcmp(ctx.current_token.value, "(") == 0 {
            b_tok := ctx.current_token
            ctx.current_token = ctx.current_token.next
            tmp_submatch = parse_matcher(ctx)

            if string.strcmp(value(ctx.current_token), ")") != 0 {
                ctx.err = errors.from(b_tok, "Bracket not closed")
                return (Matcher*) null
            }
            ctx.current_token = ctx.current_token.next
        } elif string.strcmp(ctx.current_token.type, "WORD") == 0 | string.strcmp(ctx.current_token.type, "STRING") == 0 {
            tmp_submatch = parse_single_token(ctx, ctx.current_token)
            ctx.current_token = next(ctx.current_token)
        } elif string.strcmp(ctx.current_token.value, "|") == 0 {
            ctx.current_token = next(ctx.current_token)
            m.submatcher.alternative = parse_matcher(ctx)
            break
        } else {
            break
        }

        nv := value(ctx.current_token)
        if string.strcmp(nv, "+") == 0 {
            tmp_submatch.times = '+'
            ctx.current_token = next(ctx.current_token)
        } elif string.strcmp(nv, "*") == 0 {
            tmp_submatch.times = '*'
            ctx.current_token = next(ctx.current_token)
        } elif string.strcmp(nv, "?") == 0 {
            tmp_submatch.times = '?'
            ctx.current_token = next(ctx.current_token)
        }

        append_submatch(m, tmp_submatch)

    }

    return m
}


Matcher* parse_single_token(ParsingContext* ctx, lexer.Token* t) {
    if t == null {
        return (Matcher*) null
    }
    m := new_matcher()
    m.value = t.value
    if string.strcmp(type(t), "STRING") == 0 {
        m.type = TYPE_VALUE
    } elif string.is_lower(value(t)) {
        m.type = TYPE_ALIAS
    } elif string.is_upper(value(t)) {
        m.type = TYPE_TYPE
    } else {
        ctx.err = errors.from(t, "Some characters are not allowed in this rule name")
        return (Matcher*) null
    }
    return m
}


Rule* parse_rule(ParsingContext* ctx) {
    rule_name := ctx.current_token.value
    if string.strcmp(ctx.current_token.type, "WORD") != 0 {
        io.printf("Identifier: {type: %s, value: '%s'}\0A", ctx.current_token.type, ctx.current_token.value)
        parser_error(ctx, "rule identifier must be a WORD")
    }
    ctx.current_token = ctx.current_token.next

    transformer_name := ""
    if string.strcmp(ctx.current_token.value, "[") == 0 {
        ctx.current_token = ctx.current_token.next
        transformer_name = ctx.current_token.value
        ctx.current_token = ctx.current_token.next.next
    }

    if string.strcmp(ctx.current_token.value, ":") != 0 {
        io.printf(ctx.current_token.value)
        err := ""
        io.asprintf(&err, "expected : after rule name, got %s", ctx.current_token.value)
        parser_error(ctx, err)
    }
    ctx.current_token = ctx.current_token.next

    Rule* rule = (Rule*) sys.malloc(sizeof(Rule))
    rule.name = rule_name
    rule.transformer = transformer_name

    rule.matcher = parse_matcher(ctx)
    ctx.current_token = ctx.current_token
    if ctx.current_token != null {
        if string.strcmp(type(ctx.current_token), "NL") != 0 & string.strcmp(type(ctx.current_token), "EOF") != 0 {
            io.printf("[%d, %d]: rule must terminate with newline, got: '%s' %s\n", ctx.current_token.line, ctx.current_token.char_of_line, type(ctx.current_token), value(ctx.current_token))
            return (Rule*) null
        }
    }
    if rule.matcher == null {
        return (Rule*) null
    }
    return rule
}


void parser_error(ParsingContext* ctx, chr* error) {
    e := errors.from(ctx.current_token, error)
    errors.report(e)
    sys.exit(1)
}



-- a grammar is buildt by one or more rules,
-- the entire grammar describes how the ast is buildt
Rule** parse_grammar(ParsingContext* ctx) {
    int max_rules = 40
    Rule** grammar = (Rule**) sys.malloc(sys.ptr_size * max_rules)


    for i := 0; string.strcmp(ctx.current_token.type, "EOF") != 0; {
        if string.strcmp(ctx.current_token.type, "NL") != 0 {
            rule := parse_rule(ctx)
            if rule == null {
                errors.report(ctx.err)
                return (Rule**) null
            }
            grammar[i] = rule
            i = i + 1
        } else {
            ctx.current_token = ctx.current_token.next
        }
    }

    return (Rule**) grammar
}


-- basic node for 'ast' representation
alias Node = struct {
    str type,
    str value,

    str filename,
    int line,
    int char_of_line,

    Node* parent,

    Node* children,
    Node* next,      -- required for a list of child nodes
}

Node* new_node(lexer.Token* t, str type, str value) {
    node := (Node*) sys.malloc(sizeof(Node))

    node.type = type
    node.value = value
    node.line = t.line
    node.filename = t.filename
    node.char_of_line = t.char_of_line
    node.parent = null
    node.next = null
    node.children = null
    return node
}

-- append child to a node
void child_append(Node* parent, Node* child) {
    if parent.children == null {
        parent.children = child
    } else {
        c := parent.children
        for ; c.next != null; c = c.next {
        }
        c.next = child
    }
    child.next = null
}


-- remove last node's child
void child_pop(Node* parent) {
    if parent.children == null {
        -- no children to pop
        return
    }

    c := parent.children
    if c.next == null {
        -- parent node has only one child
        sys.free((chr*) c)
        parent.children = null
        return
    }

    -- retrive previous to last child
    for ; c.next.next != null; c = c.next {
    }

    sys.free((chr*) c.next)
    c.next = null
}


alias AstContext = struct {
    lexer.Token* current_token,
    Rule** grammar,
    errors.Error* err,

    int max_line,
    int max_char_of_line,
}

AstContext* next_token(AstContext* ctx) {
    ctx.current_token = ctx.current_token.next

    ct := ctx.current_token
    if ct.line > ctx.max_line {
        ctx.max_line = ct.line
        ctx.max_char_of_line = ct.char_of_line
    } elif ct.line == ctx.max_line {
        if ct.char_of_line > ctx.max_char_of_line {
            ctx.max_char_of_line = ct.char_of_line
        }
    }
    return ctx
}



Rule* find_rule(Rule** grammar, str rule_name) {
    for i := 0; true ;i = i + 1 {
        rule := grammar[i]
        if string.strcmp(rule.name, rule_name) == 0 {
            return rule
        }
    }
    return (Rule*) null
}


alias ParseResult = struct {
    errors.Error* err,
    Node* node,
}


-- start parsing the list of tokens by applying a specific rule
ParseResult* parse_to_ast(AstContext* c, Rule* rule) {
    res := (ParseResult*) sys.malloc(sizeof(ParseResult))
    res.err = null
    res.node = new_node(c.current_token, rule.name, (chr*) null)
    start_match := c.current_token

    -- start matching querys tokens
    -- res.err = parse_query(c, rule.matcher, res.node)
    res.err = execute_matcher(c, rule, rule.matcher, res.node)
    if res.err != null {
        res.err.line = c.max_line
        res.err.char_of_line = c.max_char_of_line
    }
    return res
}


-- appends match results to root.
--
errors.Error* execute_matcher(AstContext* ctx, Rule* rule, Matcher* matcher, Node* root) {
    -- io.printf("Matching: %s\n", rule.name)
    err := (errors.Error*) null
    for m := matcher; m != null; m = m.alternative {
        err = execute_matcher_straight(ctx, rule, m, root)
        if err == null {
            break
        }
    }
    return err
}

errors.Error* execute_matcher_straight(AstContext* ctx, Rule* rule, Matcher* matcher, Node* root) {
    buf := ""
    -- io.printf("Executing rule: %s '%p'\n", rule.name, matcher)

    err := (errors.Error*) null
    anchor := ctx.current_token
    child := (Node*) null

    if matcher.type == TYPE_IGNORE {
        if matcher.submatcher != null {
            err = execute_matcher(ctx, rule, matcher.submatcher, root)
        }
    } elif matcher.type == TYPE_VALUE {
        if matcher.value == null {
            io.printf("Matcher type is null\n")
        }
        buf := sys.malloc(string.strlen(matcher.value) -1)
        io.sscanf(matcher.value, `"%[^"]"`, buf)
        if string.strcmp(buf, ctx.current_token.value) != 0 {
            io.asprintf(
                &buf,
                `Unable to match token by value: %s != "%s"`,
                buf,
                ctx.current_token.value,
            )

            return errors.from(ctx.current_token, buf)
        }
        child = new_node(ctx.current_token, ctx.current_token.type, ctx.current_token.value)
        ctx = next_token(ctx)
    } elif matcher.type == TYPE_TYPE {
        if matcher.value == null {
            io.printf("Matcher type is null\n")
        }
        if string.strcmp(matcher.value, ctx.current_token.type) != 0 {
            io.asprintf(
                &buf,
                `Unable to match token by type, expecing "%s" got "%s"`,
                matcher.value,
                ctx.current_token.type,
            )
            return errors.from(ctx.current_token, buf)
        }
        child = new_node(ctx.current_token, ctx.current_token.type, ctx.current_token.value)
        ctx = next_token(ctx)
    } elif matcher.type == TYPE_ALIAS {
        alias_rule := find_rule(ctx.grammar, matcher.value)
        if alias_rule == null {
            return errors.from(ctx.current_token, "Unable to find alias rule")
        }

        parse_result := parse_to_ast(ctx, alias_rule)
        if parse_result.err != null {
            return parse_result.err
        }
        child = parse_result.node
    } else {
        sys.assert(false, "CRITICAL: Parser not implemented")
    }

    if child != null {
        child_append(root, child)
    }

    if matcher.times == '?' & err != null {
        err = null
    } elif matcher.times == '*' {
        if err != null {
            err = null
        } else {
            return execute_matcher(ctx, rule, matcher, root)
        }
    } elif matcher.times == '+' & err == null {
        err = execute_matcher(ctx, rule, matcher, root)
        if err != null {
            err = null
        } else {
            return (errors.Error*) null
        }
    }


    if matcher.next != null & err == null {
        err = execute_matcher(ctx, rule, matcher.next, root)
    }
    if err != null {
        ctx.current_token = anchor
        if child != null {
            child_pop(root)
        }
    }

    return err
}


-- Parse a list of tokens
--
-- :param grammar: list of null terminated grammar rules
-- :param start: starting rule to use for token parsing
-- :param tokens: list of tokens of the language
ParseResult* ast(Rule** grammar, str start, lexer.Token* tokens) {
    -- find starting rule
    Rule* start_matcher = find_rule(grammar, start)
    sys.assert(start_matcher != null, "Unable to find starting rule in the grammar")

    AstContext c
    c.current_token = tokens
    c.grammar = grammar

    res := parse_to_ast(&c, start_matcher)
    if string.strcmp(c.current_token.type, "EOF") != 0 & res.err == null {
        res.err = errors.from(c.current_token, "SyntaxError: parsing ended here")
    }

    return res
}
